{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47742e8a-bdb4-4d44-968e-96a68f17ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import h5py\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import re\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda6672-e7fb-4962-a433-096f8b143379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'glob' function to find all files with the extension '.hdf5' \n",
    "my_dir = glob('project_data/LH014/hdf5/*hdf5')\n",
    "# Sort the list of file paths in ascending order.\n",
    "my_dir = sorted(my_dir)\n",
    "my_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddaffb2-f962-499d-adb7-48d7e867af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the width of each channel by dividing the total bandwidth (19.600) \n",
    "# by the number of channels (1024).\n",
    "freq_range_per_channel = 19.600 / 1024\n",
    "\n",
    "# Create a dictionary for tune1 where each key is 'channel_X' (X being the channel number from 1 to 1024)\n",
    "# and each value is the starting frequency of the channel.\n",
    "# The starting frequency for channel 1 is 20.0, and each subsequent channel's starting frequency\n",
    "# is incremented by 'freq_range_per_channel'.\n",
    "channel_dict_tune1 = {'channel_' + str(i+1): (20.0 + i * freq_range_per_channel) for i in range(1024)}\n",
    "\n",
    "# Create a similar dictionary for tune2 where the starting frequency for channel 1 is 28.0,\n",
    "# and each subsequent channel's starting frequency is incremented by 'freq_range_per_channel'.\n",
    "channel_dict_tune2 = {'channel_' + str(i+1): (28.0 + i * freq_range_per_channel) for i in range(1024)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0d31f-cabb-433a-b652-1099cd8adbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(h5py.File('project_data/LH014/hdf5/111.hdf5')['Observation1']['Tuning1']['V'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6510c08-a394-4beb-ac8f-df88bd589f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten HDF5 data and save it as Parquet files\n",
    "def flatten_to_parquet(session_number):\n",
    "    # Construct the filename based on the session number\n",
    "    filename = 'project_data/LH014/hdf5/' + str(session_number) + '.hdf5'\n",
    "    \n",
    "    # Load the 'time' data from the HDF5 file and convert it to a 2D array\n",
    "    t = np.array(h5py.File(filename)['Observation1']['time'])\n",
    "    t = t[:, np.newaxis]\n",
    "    \n",
    "    # Iterate over the tunings\n",
    "    for tune in ['Tuning1', 'Tuning2']:\n",
    "        # Iterate over the polarizations\n",
    "        for pol in ['I', 'V']:\n",
    "            \n",
    "            # Load the data for the current tuning and polarization from the HDF5 file\n",
    "            data = np.array(h5py.File(filename)['Observation1'][tune][pol])\n",
    "            # Concatenate the data with the time array along the second axis\n",
    "            np_arr = np.concatenate((data, t), axis=1)\n",
    "            \n",
    "            # Create a dictionary where keys are 'channel_X' and 'time',\n",
    "            # and values are the corresponding columns of the concatenated array\n",
    "            my_dict = {'channel_' + str(i+1): np_arr[:, i] for i in range(1024)} | {'time': np_arr[:, -1]}\n",
    "            # Create a PyArrow table from the dictionary\n",
    "            pa_table = pa.table(my_dict)\n",
    "            \n",
    "            counter = 0\n",
    "            # Convert the PyArrow table to batches with a maximum chunk size of 50,000\n",
    "            for batch in pa_table.to_batches(max_chunksize=50_000):\n",
    "                # Convert the batch to a pandas DataFrame\n",
    "                df = batch.to_pandas()\n",
    "                # Transform the DataFrame from wide to long format\n",
    "                df = df.melt(id_vars=['time'], var_name=['channel'])\n",
    "                # Add 'tuning' and 'polarization' columns to the DataFrame\n",
    "                df['tuning'] = tune\n",
    "                df['polarization'] = pol\n",
    "                # Map the 'channel' column to the corresponding frequency based on the tuning\n",
    "                if tune == 'Tuning1':\n",
    "                    df['frequency'] = df.channel.map(channel_dict_tune1)\n",
    "                else:\n",
    "                    df['frequency'] = df.channel.map(channel_dict_tune2)\n",
    "                # Save the DataFrame as a Parquet file\n",
    "                df.to_parquet(f'project_data/LH014/parquets/{str(session_number)}_{tune}_{pol}_{counter}.parquet')\n",
    "                counter += 1\n",
    "                # Print the current status\n",
    "                print((tune, pol, counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c0d46-0efe-408e-8acb-3157962cf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flatten_to_parquet(241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee80c70-2199-4bcc-850a-463e738aab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a sorted list of all filenames in the 'Project_data/LH014/parquets/' directory\n",
    "# that match the pattern '*_Tuning1_I_1*'.\n",
    "done_dir = sorted(glob('Project_data/LH014/parquets/*_Tuning1_I_1*'))\n",
    "\n",
    "# Define a regular expression pattern to extract the session number from the filename.\n",
    "pattern_done = r\"parquets/(\\d+)_Tuning1\"\n",
    "\n",
    "# Use a list comprehension to apply the regular expression pattern to each filename in 'done_dir'.\n",
    "# Extract the session number (as an integer) from each filename.\n",
    "done_dir = [int(re.findall(pattern_done, x)[0]) for x in done_dir]\n",
    "\n",
    "# Remove duplicates by converting the list to a set and then back to a list.\n",
    "done_dir = list(set(done_dir))\n",
    "done_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cb9b0-f152-4d1d-88db-f99b262f2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to extract the session number from the filename.\n",
    "pattern = r\"hdf5\\/(\\d+)\\.hdf5\"\n",
    "\n",
    "# Use a list comprehension to apply the regular expression pattern to each filename in 'my_dir'.\n",
    "# Extract the session number (as an integer) from each filename.\n",
    "to_do = [int(re.findall(pattern, x)[0]) for x in my_dir]\n",
    "\n",
    "# Filter the list 'to_do' to exclude any session numbers that are already in 'done_dir'.\n",
    "to_do = [x for x in to_do if x not in done_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841d27e8-13d4-4cd4-8263-47ca9b09e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for x in to_do:\n",
    "    print(x)\n",
    "    flatten_to_parquet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb79cd-422d-4af8-88a2-89c95cf865bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g'\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Your App Name\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    # .config(\"spark.executor.memory\", \"3g\")\n",
    "    # .config(\"spark.executor.cores\", \"7\")\n",
    "    # .config(\"spark.executor.memory\", \"8g\")\n",
    "    \n",
    "    .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f198c82-26a7-4ec4-9ff2-6cac4a26d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('project_data/LH014/parquets/111*.parquet')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127b13c-bc09-4319-8841-41d4ab09610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47ff95-f46f-4feb-af25-f9d7277469ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fb436-18ac-44d4-834f-60ecb19a4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('channel').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d660001-90aa-410b-a087-5d8b69a5785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('frequency').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee938e7-8774-4095-a100-550fc043815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('polarization').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f28a72-588f-4d79-b614-f9917e33557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('tuning').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc21e3-1696-4a9e-8e2f-ebd0828ccfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de11ce3-4167-4adc-bb39-b44d60493c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d2b99-4208-4dd3-aaeb-5595021f3569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2030c-ca09-4a3a-a558-6f7251ee00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f014c-8f6c-4e4e-a364-a41ed3d68b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
