{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61c28c6-a791-43aa-bc4d-265fb87a37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler, VectorAssembler\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9adf6e50-cfd9-4742-85ae-183998910490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAJnRFWHRUaXRsZQB0cnVuYyhyb2NrZXQsMC4wMCwwLjgwKSBjb2xvcm1hcNC/h8UAAAAsdEVYdERlc2NyaXB0aW9uAHRydW5jKHJvY2tldCwwLjAwLDAuODApIGNvbG9ybWFwgeH1YgAAADB0RVh0QXV0aG9yAE1hdHBsb3RsaWIgdjMuOC4yLCBodHRwczovL21hdHBsb3RsaWIub3JnDxXeEgAAADJ0RVh0U29mdHdhcmUATWF0cGxvdGxpYiB2My44LjIsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmchs0E1AAACKElEQVR4nO3WS27jMBRFQZLoTWT/C1UGsSWY5LPU41s16EDi106APn38+zlaa6333lpr7e/f6/n9Zh4f5/N+vBfj4+G867nt92nP5pX3a/N9vu93zRv7zzGtX/ctPu/r/ajuPe9/fv79ufM5t/PO+36fN6r73Jw3lvXzvP1+63nVOfvzyn2Pp+ueja/f7/T+qOa/n/vHvW7PWe5f7Hs37zi+rq/vfXyMr/c9bs4/tu/PdfM5xfrzZy/m9+nz9WJ8Xj+/v1vXb9bNz+P/1r2/yHLf6Ytezhn7c9v53D7GW7XvMn5+Idv95j+Mfv2iXuN9mt8fzb/WFeO9b5/7Mj7ff3zOez0v+xb7n+vv5i3vx6P5fcy/sKf7VOte/481ACCOAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAL9AmeDKoOnouM/AAAAAElFTkSuQmCC",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>trunc(rocket,0.00,0.80)</strong> </div><div class=\"cmap\"><img alt=\"trunc(rocket,0.00,0.80) colormap\" title=\"trunc(rocket,0.00,0.80)\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAJnRFWHRUaXRsZQB0cnVuYyhyb2NrZXQsMC4wMCwwLjgwKSBjb2xvcm1hcNC/h8UAAAAsdEVYdERlc2NyaXB0aW9uAHRydW5jKHJvY2tldCwwLjAwLDAuODApIGNvbG9ybWFwgeH1YgAAADB0RVh0QXV0aG9yAE1hdHBsb3RsaWIgdjMuOC4yLCBodHRwczovL21hdHBsb3RsaWIub3JnDxXeEgAAADJ0RVh0U29mdHdhcmUATWF0cGxvdGxpYiB2My44LjIsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmchs0E1AAACKElEQVR4nO3WS27jMBRFQZLoTWT/C1UGsSWY5LPU41s16EDi106APn38+zlaa6333lpr7e/f6/n9Zh4f5/N+vBfj4+G867nt92nP5pX3a/N9vu93zRv7zzGtX/ctPu/r/ajuPe9/fv79ufM5t/PO+36fN6r73Jw3lvXzvP1+63nVOfvzyn2Pp+ueja/f7/T+qOa/n/vHvW7PWe5f7Hs37zi+rq/vfXyMr/c9bs4/tu/PdfM5xfrzZy/m9+nz9WJ8Xj+/v1vXb9bNz+P/1r2/yHLf6Ytezhn7c9v53D7GW7XvMn5+Idv95j+Mfv2iXuN9mt8fzb/WFeO9b5/7Mj7ff3zOez0v+xb7n+vv5i3vx6P5fcy/sKf7VOte/481ACCOAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAIJAAAIJAAAIJAAAIBAAgAAAgkAAAgkAAAgkAAAgEACAAACCQAACCQAACCQAACAQAIAAAL9AmeDKoOnouM/AAAAAElFTkSuQmCC\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#03051aff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #03051aff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#000000ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #000000ff;\"></div></div><div style=\"float: right;\">over <div title=\"#f69c73ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #f69c73ff;\"></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.LinearSegmentedColormap at 0x12d34af90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to truncate a colormap.\n",
    "# 'cmap' is the original colormap.\n",
    "# 'minval' and 'maxval' are the range values to truncate the colormap.\n",
    "# 'n' is the number of discrete colors to use in the new colormap.\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    # Create a new colormap by truncating the original colormap.\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "# Get the 'rocket' colormap from matplotlib's colormap library.\n",
    "cmap = plt.get_cmap('rocket')\n",
    "\n",
    "# Truncate the 'rocket' colormap to only include colors in the range [0, 0.8].\n",
    "new_cmap = truncate_colormap(cmap, 0, 0.8)\n",
    "\n",
    "# Set the color for bad (masked) values to black in the new colormap.\n",
    "new_cmap.set_bad('black')\n",
    "\n",
    "# Output the truncated colormap.\n",
    "new_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf934ad0-8a7b-4f4e-80a0-5635fed8ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441d74f-7ce1-426a-a5d3-92590508940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function 'ss' to perform data processing and scaling for a given session, tuning, and polarization.\n",
    "def ss(session, tun, pol):\n",
    "    # Read Parquet files for the specified session, tuning, and polarization into a Spark DataFrame.\n",
    "    temp_df = spark.read.parquet(f'project_data/LH014/parquets/{session}_{tun}_{pol}*.parquet')\n",
    "    \n",
    "    # Process the DataFrame:\n",
    "    # - Cast the 'time' column to integer and store it as 'seconds'.\n",
    "    # - Round the 'frequency' column to 2 decimal places and store it as 'frequency2'.\n",
    "    # - Group by 'seconds' and 'frequency2', and aggregate the 'value' column by summing it.\n",
    "    temp_df = (temp_df\n",
    "               .withColumn('seconds', col('time').cast('int'))\n",
    "               .withColumn('frequency2', round('frequency', 2))\n",
    "               .groupby('seconds', 'frequency2').agg(sum('value').alias('value'))\n",
    "              )\n",
    "    \n",
    "    # Assemble the 'value' column into a feature vector named 'value_vec'.\n",
    "    assembler = VectorAssembler(inputCols=['value'], outputCol='value_vec')\n",
    "    df_assembled = assembler.transform(temp_df)\n",
    "    \n",
    "    # Initialize the StandardScaler to scale the 'value_vec' column.\n",
    "    scaler = StandardScaler(inputCol='value_vec', outputCol='scaled_value', withStd=True, withMean=True)\n",
    "    \n",
    "    # Fit the scaler to the DataFrame and transform the DataFrame.\n",
    "    scaler_model = scaler.fit(df_assembled)\n",
    "    df_scaled = scaler_model.transform(df_assembled)\n",
    "    \n",
    "    # Define a UDF to convert the scaled value vector to a float.\n",
    "    to_float = udf(lambda vector: float(vector[0]))\n",
    "    \n",
    "    # Apply the UDF to the 'scaled_value' column and drop the 'value_vec' column.\n",
    "    df_scaled = df_scaled.withColumn('scaled_value', to_float('scaled_value')).drop('value_vec')\n",
    "    \n",
    "    # Write the scaled DataFrame to Parquet files in overwrite mode.\n",
    "    df_scaled.write.mode('overwrite').parquet('project_data/LH014/figs/temp/')\n",
    "    \n",
    "    # Read the temporary Parquet files into a list of Pandas DataFrames.\n",
    "    temp_files = glob.glob('project_data/LH014/figs/temp/*.parquet')\n",
    "    temp_df = [pd.read_parquet(file) for file in temp_files]\n",
    "    \n",
    "    # Concatenate the list of DataFrames into a single DataFrame.\n",
    "    temp_df = pd.concat(temp_df)\n",
    "    \n",
    "    # Write the final DataFrame to a Parquet file.\n",
    "    temp_df.to_parquet(f'project_data/LH014/figs/{session}_{tun}_{pol}_SS.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ae58f-eda0-437c-9f2e-8888c16d0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s = '141'\n",
    "ss(s,'Tuning2','V')\n",
    "ss(s,'Tuning2','I')\n",
    "ss(s,'Tuning1','I') \n",
    "ss(s,'Tuning1','V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00385da3-5d71-4f04-b6db-21cbd13086af",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('project_data/LH014/figs/*_SS.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9228e0-24fd-4732-a3c3-ddd28109fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function 'ss_fig' to create and save a heatmap for a given session, tuning, and polarization.\n",
    "def ss_fig(session, tun, pol):\n",
    "    # Read the Parquet file for the specified session, tuning, and polarization into a Pandas DataFrame.\n",
    "    temp_df = pd.read_parquet(f'project_data/LH014/figs/{session}_{tun}_{pol}_SS.parquet')\n",
    "    \n",
    "    # Ensure the 'scaled_value' column is of type float.\n",
    "    temp_df.scaled_value = temp_df.scaled_value.astype(float)\n",
    "    \n",
    "    # Get the start time in a readable format.\n",
    "    t1 = pd.to_datetime(temp_df.seconds.iloc[0], unit='s').strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    # Convert the 'seconds' column to datetime format and store it in a new column 'time'.\n",
    "    temp_df['time'] = pd.to_datetime(temp_df.seconds, unit='s')\n",
    "    \n",
    "    # Extract the time part (HH:MM:SS) from the 'time' column and store it in a new column 'time2'.\n",
    "    temp_df['time2'] = temp_df.time.dt.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    # Create a pivot table with 'frequency2' as rows, 'time2' as columns, and 'scaled_value' as values.\n",
    "    heatmap_data = temp_df.pivot('frequency2', 'time2', 'scaled_value')\n",
    "    \n",
    "    # Sort the pivot table by 'frequency2' in descending order.\n",
    "    heatmap_data.sort_values('frequency2', ascending=False, inplace=True)\n",
    "    \n",
    "    # Create a figure and axis for the heatmap.\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Create the heatmap using seaborn.\n",
    "    sns.heatmap(heatmap_data, cmap=new_cmap, ax=ax, cbar_kws={'label': 'StandardScaler'}, vmin=-1, vmax=1)\n",
    "    \n",
    "    # Set the title for the heatmap.\n",
    "    title = f'Session {session}, {tun}, Stokes: {pol}, Date: {t1}'\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Adjust the number of bins for the y-axis and x-axis.\n",
    "    ax.locator_params(axis='y', nbins=10)\n",
    "    ax.locator_params(axis='x', nbins=10)\n",
    "    \n",
    "    # Rotate the x-axis labels for better readability.\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Set the labels for the y-axis and x-axis.\n",
    "    ax.set_ylabel('Frequency MHz')\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "    # Save the heatmap as a PNG file.\n",
    "    plt.savefig(f'project_data/LH014/figs/{session}_{tun}_{pol}_SS.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c99c3a-b9d6-478b-8b51-1e7a9ffa6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ss_fig('project_data/LH014/figs/101_Tuning2_V_SS.parquet')\n",
    "ss_fig('261','Tuning2','V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae567a3-2d75-49d5-8327-7e914627890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function 'ss_fig_bands' to create and save heatmaps for a given session, tuning, and polarization.\n",
    "def ss_fig_bands(session, tun, pol):\n",
    "    # Read the Parquet file for the specified session, tuning, and polarization into a Pandas DataFrame.\n",
    "    temp_df = pd.read_parquet(f'project_data/LH014/figs/{session}/{session}_{tun}_{pol}_SS.parquet')\n",
    "    \n",
    "    # Ensure the 'scaled_value' column is of type float.\n",
    "    temp_df.scaled_value = temp_df.scaled_value.astype(float)\n",
    "    \n",
    "    # Get the start time in a readable format.\n",
    "    t1 = pd.to_datetime(temp_df.seconds.iloc[0], unit='s').strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    # Convert the 'seconds' column to datetime format and store it in a new column 'time'.\n",
    "    temp_df['time'] = pd.to_datetime(temp_df.seconds, unit='s')\n",
    "    \n",
    "    # Extract the time part (HH:MM:SS) from the 'time' column and store it in a new column 'time2'.\n",
    "    temp_df['time2'] = temp_df.time.dt.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    # Create a pivot table with 'frequency2' as rows, 'time2' as columns, and 'scaled_value' as values.\n",
    "    heatmap_data = temp_df.pivot('frequency2', 'time2', 'scaled_value')\n",
    "    \n",
    "    # Sort the pivot table by 'frequency2' in descending order.\n",
    "    heatmap_data.sort_values('frequency2', ascending=False, inplace=True)\n",
    "    \n",
    "    # Generate heatmaps for different value ranges.\n",
    "    for i in [-0.9, -0.7, -0.5, -0.3]:\n",
    "        # Create a figure and axis for the heatmap.\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Create the heatmap using seaborn.\n",
    "        sns.heatmap(heatmap_data, cmap=new_cmap, ax=ax, cbar_kws={'label': 'StandardScaler'}, vmin=i, vmax=1)\n",
    "        \n",
    "        # Set the title for the heatmap.\n",
    "        title = f'Session {session}, {tun}, Stokes: {pol}, Date: {t1}'\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        # Adjust the number of bins for the y-axis and x-axis.\n",
    "        ax.locator_params(axis='y', nbins=10)\n",
    "        ax.locator_params(axis='x', nbins=10)\n",
    "        \n",
    "        # Rotate the x-axis labels for better readability.\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Set the labels for the y-axis and x-axis.\n",
    "        ax.set_ylabel('Frequency MHz')\n",
    "        ax.set_xlabel('')\n",
    "        \n",
    "        # Save the heatmap as a PNG file.\n",
    "        plt.savefig(f'project_data/LH014/figs/{session}_{tun}_{pol}_SS_{str(i)}.png')\n",
    "        \n",
    "        # Close the figure\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab491a-b648-4d3c-aa80-213cab71fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s = '271'\n",
    "ss_fig_bands(s,'Tuning1','I')\n",
    "ss_fig_bands(s,'Tuning1','V')\n",
    "ss_fig_bands(s,'Tuning2','I')\n",
    "ss_fig_bands(s,'Tuning2','V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633a23d-6697-4e8d-90cc-06665d130073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "to_do = ['141', '151']\n",
    "\n",
    "##'191'] ##'181' heat_map errors with these two\n",
    "for s in to_do:\n",
    "    print(s)\n",
    "    ss_fig_bands(s,'Tuning1','I')\n",
    "    ss_fig_bands(s,'Tuning1','V')\n",
    "    ss_fig_bands(s,'Tuning2','I')\n",
    "    ss_fig_bands(s,'Tuning2','V')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787e8c8-a5ee-484c-96e7-f7b217d9b51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa06ac-03aa-4d43-b454-e6a5cc5f2950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b44460-6a03-4593-87fe-797f5e1fcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8702c-4f2d-4877-b23a-8a0bd70df3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
